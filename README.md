# GPU-Aware Distributed Job Dispatcher

A Python-based job dispatcher that intelligently assigns compute tasks to GPUs based on real-time memory availability, with persistent job tracking and recovery capabilities.

## ğŸš€ Features

- ğŸ¯ **GPU-aware job scheduling** based on free memory
- ğŸ“ **Persistent job tracking** using JSONL files
- ğŸ”„ **Automatic state recovery** after crashes/restarts
- ğŸ“Š **Real-time job status monitoring**
- ğŸ“ **Per-job log files** with stdout/stderr capture
- âš™ï¸ **Configurable thresholds and GPU selection**

---

## ğŸ“¦ Installation

1. Install prerequisites:
   ```bash
   pip install pynvml pyyaml
   ```

2. Clone/download the dispatcher script:
   ```bash
   git clone [your-repo-url-here]
   ```

---

## âš¡ Quick Start

1. Create a commands file (`commands.txt`):
   ```txt
   python train.py --batch-size 128
   python inference.py --input-dir ./data
   ```

2. Run the dispatcher:
   ```bash
   python cudaq.py run --commands-file commands.txt --min-free-mem-mb 8000
   ```

3. Check status:
   ```bash
   python cudaq.py status
   ```

---

## âš™ï¸ Configuration

Create `config.yaml` for persistent settings:
```yaml
gpu_ids: [0, 1, 2]          # Which GPUs to use
min_free_mem_mb: 10000      # Minimum free memory required
poll_interval: 30           # Check interval in seconds
commands_file: jobs.txt     # Job commands source
log_dir: ./job_logs         # Log storage location
jobs_file: ./queue.jsonl    # Job tracking file
```

Start dispatcher with config:
```bash
python cudaq.py run --config config.yaml
```

---

## ğŸ“‹ Job Management

### Command File Format

Plain text file with one command per line:
```txt
# comments start with #
python train_resnet.py
python process_data.py --workers 4
```

### Job Lifecycle

- **Pending**: Waiting for GPU resources  
- **Running**: Actively executing on GPU  
- **Completed**: Finished successfully  
- **Failed**: Exited with error/crash  

---

## ğŸ’¾ Persistent Tracking

Jobs are tracked in JSONL format with:
- PID and start time
- Assigned GPU ID
- Status history
- Log file path
- Full command string

---

## ğŸ“ Log Files

- Automatically created in `log_dir`
- Format: `job_YYYYMMDD_HHMMSS.log`
- Contains full stdout/stderr output
- Path stored in job tracking file

---

## ğŸ” Status Monitoring

View current job states:
```bash
python cudaq.py status
```

Sample output:
```
[â†’] python train.py     â†’ GPU 0 â†’ Running (PID 1234)
[âœ“] python infer.py     â†’ GPU 1 â†’ Completed
[ ]  python eval.py     â†’ GPU N/A â†’ Pending
```

---

## ğŸ“„ License

MIT License â€“ see header in source code for details.
```
